{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Training Notebook\n",
                "## AI Model Web Integration - Regression Model\n",
                "\n",
                "This notebook:\n",
                "1. Loads and preprocesses data\n",
                "2. Trains multiple regression models (Linear, Ridge, Lasso, ElasticNet)\n",
                "3. Performs cross-validation and hyperparameter tuning\n",
                "4. Saves the best model and preprocessing objects"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Import libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.datasets import fetch_california_housing\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
                "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
                "import joblib\n",
                "import os\n",
                "\n",
                "# Set random state for reproducibility\n",
                "RANDOM_STATE = 42\n",
                "np.random.seed(RANDOM_STATE)\n",
                "\n",
                "# Display settings\n",
                "pd.set_option('display.max_columns', None)\n",
                "sns.set_style('darkgrid')\n",
                "%matplotlib inline"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Load California Housing dataset\n",
                "print(\"Loading California Housing dataset...\")\n",
                "data = fetch_california_housing()\n",
                "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
                "y = pd.Series(data.target, name='MedHouseVal')\n",
                "\n",
                "print(f\"Dataset shape: {X.shape}\")\n",
                "print(f\"\\nFeatures: {list(X.columns)}\")\n",
                "print(f\"\\nFirst few rows:\")\n",
                "X.head()"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Basic statistics\n",
                "print(\"Dataset Statistics:\")\n",
                "X.describe()"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Introduce Missing Values (for demonstration)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Introduce some missing values randomly (5% of data)\n",
                "print(\"Introducing missing values...\")\n",
                "X_with_missing = X.copy()\n",
                "np.random.seed(RANDOM_STATE)\n",
                "\n",
                "for col in X_with_missing.columns:\n",
                "    mask = np.random.random(len(X_with_missing)) < 0.05\n",
                "    X_with_missing.loc[mask, col] = np.nan\n",
                "\n",
                "print(\"\\nMissing values per column:\")\n",
                "print(X_with_missing.isnull().sum())\n",
                "print(f\"\\nTotal missing values: {X_with_missing.isnull().sum().sum()}\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Train-Test Split"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Split data: 80% train, 20% test\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X_with_missing, y,\n",
                "    test_size=0.2,\n",
                "    random_state=RANDOM_STATE\n",
                ")\n",
                "\n",
                "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
                "print(f\"Test set size: {X_test.shape[0]} samples\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Initialize preprocessing objects\n",
                "imputer = SimpleImputer(strategy='mean')\n",
                "scaler = StandardScaler()\n",
                "\n",
                "# Fit and transform training data\n",
                "print(\"Preprocessing training data...\")\n",
                "X_train_imputed = imputer.fit_transform(X_train)\n",
                "X_train_scaled = scaler.fit_transform(X_train_imputed)\n",
                "\n",
                "# Transform test data\n",
                "print(\"Preprocessing test data...\")\n",
                "X_test_imputed = imputer.transform(X_test)\n",
                "X_test_scaled = scaler.transform(X_test_imputed)\n",
                "\n",
                "print(\"\\n‚úÖ Preprocessing complete!\")\n",
                "print(f\"Training data shape: {X_train_scaled.shape}\")\n",
                "print(f\"Test data shape: {X_test_scaled.shape}\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Training & Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Define models to compare\n",
                "models = {\n",
                "    'Linear Regression': LinearRegression(),\n",
                "    'Ridge': Ridge(random_state=RANDOM_STATE),\n",
                "    'Lasso': Lasso(random_state=RANDOM_STATE),\n",
                "    'ElasticNet': ElasticNet(random_state=RANDOM_STATE)\n",
                "}\n",
                "\n",
                "# Store results\n",
                "results = []\n",
                "\n",
                "print(\"Training and evaluating models...\\n\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f\"\\n{name}:\")\n",
                "    print(\"-\" * 40)\n",
                "    \n",
                "    # Cross-validation on training data\n",
                "    cv_scores = cross_val_score(\n",
                "        model, X_train_scaled, y_train,\n",
                "        cv=5, scoring='r2', n_jobs=-1\n",
                "    )\n",
                "    \n",
                "    # Train on full training set\n",
                "    model.fit(X_train_scaled, y_train)\n",
                "    \n",
                "    # Predictions\n",
                "    y_train_pred = model.predict(X_train_scaled)\n",
                "    y_test_pred = model.predict(X_test_scaled)\n",
                "    \n",
                "    # Calculate metrics\n",
                "    train_r2 = r2_score(y_train, y_train_pred)\n",
                "    test_r2 = r2_score(y_test, y_test_pred)\n",
                "    test_mae = mean_absolute_error(y_test, y_test_pred)\n",
                "    test_mse = mean_squared_error(y_test, y_test_pred)\n",
                "    test_rmse = np.sqrt(test_mse)\n",
                "    \n",
                "    # Store results\n",
                "    results.append({\n",
                "        'Model': name,\n",
                "        'CV Mean R¬≤': cv_scores.mean(),\n",
                "        'CV Std R¬≤': cv_scores.std(),\n",
                "        'Train R¬≤': train_r2,\n",
                "        'Test R¬≤': test_r2,\n",
                "        'Test MAE': test_mae,\n",
                "        'Test RMSE': test_rmse\n",
                "    })\n",
                "    \n",
                "    # Print results\n",
                "    print(f\"Cross-validation R¬≤ (mean ¬± std): {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
                "    print(f\"Training R¬≤: {train_r2:.4f}\")\n",
                "    print(f\"Test R¬≤: {test_r2:.4f}\")\n",
                "    print(f\"Test MAE: {test_mae:.4f}\")\n",
                "    print(f\"Test RMSE: {test_rmse:.4f}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Create results DataFrame\n",
                "results_df = pd.DataFrame(results)\n",
                "results_df = results_df.round(4)\n",
                "print(\"\\nModel Comparison Results:\")\n",
                "results_df"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Hyperparameter Tuning (Best Model)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Select best model based on test R¬≤ score\n",
                "best_model_name = results_df.loc[results_df['Test R¬≤'].idxmax(), 'Model']\n",
                "print(f\"Best model: {best_model_name}\\n\")\n",
                "\n",
                "# Hyperparameter tuning for Ridge (usually performs well)\n",
                "print(\"Performing hyperparameter tuning for Ridge Regression...\")\n",
                "param_grid = {\n",
                "    'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
                "}\n",
                "\n",
                "grid_search = GridSearchCV(\n",
                "    Ridge(random_state=RANDOM_STATE),\n",
                "    param_grid,\n",
                "    cv=5,\n",
                "    scoring='r2',\n",
                "    n_jobs=-1,\n",
                "    verbose=1\n",
                ")\n",
                "\n",
                "grid_search.fit(X_train_scaled, y_train)\n",
                "\n",
                "print(f\"\\nBest parameters: {grid_search.best_params_}\")\n",
                "print(f\"Best CV R¬≤ score: {grid_search.best_score_:.4f}\")\n",
                "\n",
                "# Get best model\n",
                "best_model = grid_search.best_estimator_"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Final Model Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Evaluate best model\n",
                "y_train_pred_final = best_model.predict(X_train_scaled)\n",
                "y_test_pred_final = best_model.predict(X_test_scaled)\n",
                "\n",
                "train_r2_final = r2_score(y_train, y_train_pred_final)\n",
                "test_r2_final = r2_score(y_test, y_test_pred_final)\n",
                "test_mae_final = mean_absolute_error(y_test, y_test_pred_final)\n",
                "test_rmse_final = np.sqrt(mean_squared_error(y_test, y_test_pred_final))\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"FINAL MODEL PERFORMANCE\")\n",
                "print(\"=\"*80)\n",
                "print(f\"\\nModel: Ridge Regression (Optimized)\")\n",
                "print(f\"Parameters: {grid_search.best_params_}\")\n",
                "print(f\"\\nTraining R¬≤: {train_r2_final:.4f}\")\n",
                "print(f\"Test R¬≤: {test_r2_final:.4f}\")\n",
                "print(f\"Test MAE: {test_mae_final:.4f}\")\n",
                "print(f\"Test RMSE: {test_rmse_final:.4f}\")\n",
                "\n",
                "# Check for overfitting\n",
                "diff = abs(train_r2_final - test_r2_final)\n",
                "print(f\"\\nDifference between train and test R¬≤: {diff:.4f}\")\n",
                "\n",
                "if diff > 0.1:\n",
                "    print(\"‚ö†Ô∏è WARNING: Possible overfitting detected!\")\n",
                "else:\n",
                "    print(\"‚úÖ Model generalizes well!\")\n",
                "\n",
                "print(\"=\"*80)"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Visualizations"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Actual vs Predicted\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.scatter(y_test, y_test_pred_final, alpha=0.5)\n",
                "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
                "plt.xlabel('Actual Values')\n",
                "plt.ylabel('Predicted Values')\n",
                "plt.title('Actual vs Predicted Values')\n",
                "plt.grid(True)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ],
            "outputs": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Residuals\n",
                "residuals = y_test - y_test_pred_final\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.scatter(y_test_pred_final, residuals, alpha=0.5)\n",
                "plt.axhline(y=0, color='r', linestyle='--')\n",
                "plt.xlabel('Predicted Values')\n",
                "plt.ylabel('Residuals')\n",
                "plt.title('Residual Plot')\n",
                "plt.grid(True)\n",
                "plt.tight_layout()\n",
                "plt.show()"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Save Model and Preprocessing Objects"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Create model directory if it doesn't exist\n",
                "model_dir = '../backend/model'\n",
                "os.makedirs(model_dir, exist_ok=True)\n",
                "\n",
                "# Save model\n",
                "model_path = os.path.join(model_dir, 'trained_model.pkl')\n",
                "joblib.dump(best_model, model_path)\n",
                "print(f\"‚úÖ Model saved to: {model_path}\")\n",
                "\n",
                "# Save scaler\n",
                "scaler_path = os.path.join(model_dir, 'scaler.pkl')\n",
                "joblib.dump(scaler, scaler_path)\n",
                "print(f\"‚úÖ Scaler saved to: {scaler_path}\")\n",
                "\n",
                "# Save imputer\n",
                "imputer_path = os.path.join(model_dir, 'imputer.pkl')\n",
                "joblib.dump(imputer, imputer_path)\n",
                "print(f\"‚úÖ Imputer saved to: {imputer_path}\")\n",
                "\n",
                "# Save feature names\n",
                "feature_names = list(X.columns)\n",
                "features_path = os.path.join(model_dir, 'feature_names.pkl')\n",
                "joblib.dump(feature_names, features_path)\n",
                "print(f\"‚úÖ Feature names saved to: {features_path}\")\n",
                "\n",
                "print(\"\\nüéâ All objects saved successfully!\")"
            ],
            "outputs": []
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Save Validation Report"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "source": [
                "# Save results to CSV\n",
                "data_dir = '../data/processed'\n",
                "os.makedirs(data_dir, exist_ok=True)\n",
                "\n",
                "report_path = os.path.join(data_dir, 'model_validation_report.csv')\n",
                "results_df.to_csv(report_path, index=False)\n",
                "print(f\"‚úÖ Validation report saved to: {report_path}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"MODEL TRAINING COMPLETE!\")\n",
                "print(\"=\"*80)\n",
                "print(\"\\nNext steps:\")\n",
                "print(\"1. Run the Flask backend: cd backend && python app.py\")\n",
                "print(\"2. Open frontend/index.html in your browser\")\n",
                "print(\"3. Test the predictions!\")\n",
                "print(\"=\"*80)"
            ],
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}