# Project Requirements: AI Model Web Integration

## ğŸ“‹ Tá»•ng Quan Dá»± Ãn (Project Overview)

**TÃªn dá»± Ã¡n:** AI Model Web Integration Platform

**Má»¥c tiÃªu:** XÃ¢y dá»±ng má»™t á»©ng dá»¥ng web responsive cho phÃ©p ngÆ°á»i dÃ¹ng tÆ°Æ¡ng tÃ¡c vá»›i má»™t AI model cÆ¡ báº£n thÃ´ng qua giao diá»‡n web, sau Ä‘Ã³ deploy lÃªn server Ä‘á»ƒ cÃ´ng khai.

**Äá»‘i tÆ°á»£ng sá»­ dá»¥ng:** NgÆ°á»i dÃ¹ng cuá»‘i muá»‘n tráº£i nghiá»‡m kháº£ nÄƒng cá»§a AI model qua trÃ¬nh duyá»‡t web.

---

## ğŸ¯ Má»¥c TiÃªu ChÃ­nh (Main Objectives)

1. âœ… XÃ¢y dá»±ng regression model (nÃ¢ng cáº¥p tá»« Linear Regression)
2. âœ… Xá»­ lÃ½ missing data vÃ  preprocessing pipeline
3. âœ… Táº¡o má»™t trang web responsive hiá»‡n Ä‘áº¡i
4. âœ… XÃ¢y dá»±ng backend API vá»›i Python (Flask/FastAPI)
5. âœ… Deploy thÃ nh cÃ´ng lÃªn server cÃ´ng khai
6. âœ… Äáº£m báº£o model prediction chÃ­nh xÃ¡c vÃ  stable

---

## ğŸ› ï¸ CÃ´ng Nghá»‡ Sá»­ Dá»¥ng (Technology Stack)

### Backend (Python - Required)
- **Framework:** Flask hoáº·c FastAPI
  - Flask: ÄÆ¡n giáº£n, phÃ¹ há»£p cho dá»± Ã¡n nhá»
  - FastAPI: Hiá»‡n Ä‘áº¡i, há»— trá»£ async, tÃ i liá»‡u API tá»± Ä‘á»™ng
- **AI/ML Libraries:**
  - **Scikit-learn** (Primary - cho Linear Regression models)
  - **NumPy** (Array operations)
  - **Pandas** (Data manipulation vÃ  preprocessing)
  - **Joblib** hoáº·c **Pickle** (Model serialization)
- **Server:** Gunicorn hoáº·c Uvicorn
- **Environment Management:** venv hoáº·c conda

### Frontend (Responsive Web)
- **HTML5:** Cáº¥u trÃºc trang web
- **CSS3:** Styling vá»›i Flexbox/Grid cho responsive
  - Mobile-first approach
  - Breakpoints: 320px, 768px, 1024px, 1440px
- **JavaScript:** Xá»­ lÃ½ tÆ°Æ¡ng tÃ¡c vÃ  gá»i API
  - Vanilla JS hoáº·c lightweight framework
  - Fetch API Ä‘á»ƒ gá»i backend

### AI Model
- **Loáº¡i model:** Regression Model (Báº£n nÃ¢ng cáº¥p cá»§a Linear Regression)
  - Base: Linear Regression
  - Enhancements: Ridge Regression, Lasso Regression, ElasticNet, hoáº·c Polynomial Features
  - Purpose: Dá»± Ä‘oÃ¡n giÃ¡ trá»‹ liÃªn tá»¥c dá»±a trÃªn input features
- **Model Format:** 
  - Saved model (.pkl vá»›i pickle/joblib)
  - Scaler object (StandardScaler/MinMaxScaler)
  - TÃ­ch há»£p qua Python backend
- **Input Data Type:** Numerical features only (float64)
- **Data Characteristics:**
  - âœ… All columns are float64 (numerical)
  - âš ï¸ Contains missing values requiring preprocessing

### Deployment
- **Server Options:**
  - **Miá»…n phÃ­:** Render, Railway, PythonAnywhere, Heroku (limited)
  - **VPS:** DigitalOcean, AWS EC2, Google Cloud
- **Domain:** TÃ¹y chá»n (cÃ³ thá»ƒ dÃ¹ng subdomain miá»…n phÃ­)
- **HTTPS:** SSL certificate (Let's Encrypt)

---

## ğŸ“± YÃªu Cáº§u Responsive Design

### Mobile (320px - 767px)
- âœ… Single column layout
- âœ… Touch-friendly buttons (min 44px)
- âœ… Hamburger menu náº¿u cÃ³ navigation
- âœ… Font size tá»‘i thiá»ƒu 16px
- âœ… Optimized images

### Tablet (768px - 1023px)
- âœ… 2-column layout khi phÃ¹ há»£p
- âœ… Larger touch targets
- âœ… Adaptive navigation

### Desktop (1024px+)
- âœ… Multi-column layout
- âœ… Hover effects
- âœ… Full navigation bar
- âœ… Optimal content width (khÃ´ng quÃ¡ rá»™ng)

### Testing Requirements
- âœ… Test trÃªn Chrome, Firefox, Safari
- âœ… Test trÃªn iOS vÃ  Android
- âœ… Lighthouse score > 80

---

## ğŸ¨ TÃ­nh NÄƒng ChÃ­nh (Core Features)

### 1. Trang Chá»§ (Home Page)
- Giá»›i thiá»‡u vá» AI model
- Demo nhanh hoáº·c vÃ­ dá»¥
- Call-to-action rÃµ rÃ ng

### 2. Model Interface
- **Input Area:**
  - Form inputs vá»›i multiple numerical fields
  - Input validation (chá»‰ cháº¥p nháº­n sá»‘)
  - Optional: CSV file upload cho batch prediction
- **Processing:**
  - Loading indicator khi Ä‘ang xá»­ lÃ½
  - Progress feedback
- **Output Area:**
  - Predicted value (sá»‘ liÃªn tá»¥c)
  - Visualization: input features chart
  - Model confidence interval (náº¿u cÃ³)
  - Feature importance display

### 3. About/Documentation
- Giáº£i thÃ­ch model hoáº¡t Ä‘á»™ng nhÆ° tháº¿ nÃ o
- Limitations vÃ  use cases
- Technical details (optional)

### 4. API Endpoints (Backend)

```python
# Core API endpoints required:

POST /api/predict
- Input: User data (JSON, Form-data, hoáº·c File)
- Output: Prediction results (JSON)
- Status codes: 200, 400, 500

GET /api/health
- Output: Service status
- For monitoring

GET /api/model-info
- Output: Model metadata, version, capabilities
```

---

## ğŸ—ï¸ Kiáº¿n TrÃºc Há»‡ Thá»‘ng (System Architecture)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Browser   â”‚
â”‚   (Frontend)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚ HTTP/HTTPS
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Server    â”‚
â”‚  (Serve HTML/   â”‚
â”‚   CSS/JS)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Python Backend â”‚
â”‚  (Flask/FastAPI)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AI Model      â”‚
â”‚  (Loaded in     â”‚
â”‚   Memory)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Cáº¥u TrÃºc Dá»± Ãn (Project Structure)

```
project-root/
â”‚
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app.py                 # Main Flask/FastAPI app
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ model.py          # Model loading & inference
â”‚   â”‚   â”œâ”€â”€ trained_model.pkl # Saved regression model
â”‚   â”‚   â””â”€â”€ scaler.pkl        # Saved scaler object
â”‚   â”œâ”€â”€ preprocessing/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ imputer.py        # Missing value handling
â”‚   â”‚   â”œâ”€â”€ scaler.py         # Feature scaling
â”‚   â”‚   â””â”€â”€ validator.py      # Input validation
â”‚   â”œâ”€â”€ requirements.txt      # Python dependencies
â”‚   â”œâ”€â”€ config.py             # Configuration
â”‚   â””â”€â”€ utils.py              # Helper functions
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                  # Original dataset
â”‚   â”œâ”€â”€ processed/            # Preprocessed data
â”‚   â””â”€â”€ train_test/           # Split data
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb         # Exploratory Data Analysis
â”‚   â”œâ”€â”€ 02_preprocessing.ipynb # Data cleaning
â”‚   â””â”€â”€ 03_model_training.ipynb # Model development
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html            # Main page
â”‚   â”œâ”€â”€ css/
â”‚   â”‚   â””â”€â”€ style.css        # Responsive styles
â”‚   â”œâ”€â”€ js/
â”‚   â”‚   â””â”€â”€ main.js          # Frontend logic
â”‚   â””â”€â”€ assets/
â”‚       â””â”€â”€ images/          # Static assets
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py          # API tests
â”‚   â”œâ”€â”€ test_model.py        # Model tests
â”‚   â””â”€â”€ test_preprocessing.py # Preprocessing tests
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt         # All dependencies
```

---

## ğŸš€ Development Workflow

### Phase 1: Setup & Planning (Day 1-2)
- [ ] Chá»n dataset (California Housing, Diabetes, hoáº·c custom)
- [ ] Setup Python environment (Python 3.8+)
- [ ] Initialize Git repository
- [ ] Create project structure
- [ ] Install dependencies: `pip install scikit-learn pandas numpy flask`

### Phase 1.5: Data Analysis & Preprocessing (Day 3-5) âš ï¸ CRITICAL
- [ ] Load vÃ  explore dataset
- [ ] Analyze missing values pattern
- [ ] Implement missing value imputation
- [ ] Feature scaling implementation
- [ ] Outlier detection vÃ  handling
- [ ] Train-test split (80-20 hoáº·c 70-30)
- [ ] Save preprocessing objects (imputer, scaler)
- [ ] Document preprocessing steps

### Phase 2: Model Development (Day 6-8)
- [ ] Train baseline Linear Regression
- [ ] Compare vá»›i Ridge, Lasso, ElasticNet
- [ ] Evaluate metrics: MAE, MSE, RMSE, RÂ² Score
- [ ] Select best model variant
- [ ] Save model vá»›i joblib: `joblib.dump(model, 'model.pkl')`
- [ ] Create inference pipeline (preprocessing + prediction)
- [ ] Test model locally vá»›i sample data

### Phase 2.5: Model Validation (Day 9-10) âš ï¸ CRITICAL
- [ ] Perform k-fold cross-validation (k=5 or k=10)
- [ ] Compare models using cross-validation scores
- [ ] Hyperparameter tuning vá»›i GridSearchCV/RandomizedSearchCV
- [ ] Validate on hold-out test set
- [ ] Check for overfitting/underfitting
- [ ] Generate validation report vá»›i metrics
- [ ] Ensure reproducibility (random_state=42 everywhere)

### Phase 3: Backend Development (Week 2)
- [ ] Setup Flask/FastAPI
- [ ] Create API endpoints
- [ ] Integrate model with API
- [ ] Add error handling
- [ ] Test API vá»›i Postman/curl

### Phase 4: Frontend Development (Week 2-3)
- [ ] Design UI/UX mockup
- [ ] Develop HTML structure
- [ ] Create responsive CSS
- [ ] Implement JavaScript API calls
- [ ] Add loading states & error handling
- [ ] Cross-browser testing

### Phase 5: Integration Testing (Week 3)
- [ ] Test frontend + backend integration
- [ ] Performance testing
- [ ] Responsive design testing
- [ ] Bug fixes

### Phase 6: Deployment (Week 3-4)
- [ ] Chá»n hosting platform
- [ ] Setup server environment
- [ ] Configure environment variables
- [ ] Deploy backend
- [ ] Deploy frontend
- [ ] Setup domain (optional)
- [ ] Enable HTTPS
- [ ] Final testing on production

### Phase 7: Documentation (Week 4)
- [ ] User documentation
- [ ] Technical documentation
- [ ] README with setup instructions
- [ ] Demo video (optional)

---

## ğŸ”’ YÃªu Cáº§u Báº£o Máº­t (Security Requirements)

- âœ… Input validation vÃ  sanitization
- âœ… Rate limiting trÃªn API
- âœ… CORS configuration Ä‘Ãºng
- âœ… Environment variables cho sensitive data
- âœ… HTTPS trong production
- âœ… KhÃ´ng expose model weights cÃ´ng khai

---

## ğŸ“Š YÃªu Cáº§u Hiá»‡u NÄƒng (Performance Requirements)

- âœ… API response time < 3 seconds
- âœ… Page load time < 2 seconds
- âœ… Mobile-friendly (Google Mobile-Friendly Test)
- âœ… Lighthouse Performance Score > 80
- âœ… Model inference time < 2 seconds

---

## ğŸ“ Deliverables (Sáº£n Pháº©m BÃ n Giao)

1. **Source Code:**
   - Complete codebase trÃªn GitHub
   - Clean, commented code
   - Git history rÃµ rÃ ng

2. **Deployed Application:**
   - Live URL cÃ³ thá»ƒ truy cáº­p
   - Working demo

3. **Documentation:**
   - README.md vá»›i setup instructions
   - API documentation
   - User guide

4. **Presentation:**
   - Demo slides/video
   - Technical explanation
   - Challenges & solutions

---

## ğŸ“ TiÃªu ChÃ­ ÄÃ¡nh GiÃ¡ (Evaluation Criteria)

| TiÃªu chÃ­ | Trá»ng sá»‘ | MÃ´ táº£ |
|----------|----------|-------|
| **Functionality** | 30% | Model hoáº¡t Ä‘á»™ng Ä‘Ãºng, API stable |
| **Code Quality** | 20% | Clean code, best practices, Python standards |
| **Responsive Design** | 20% | Mobile/tablet/desktop friendly |
| **Deployment** | 15% | Successfully deployed vÃ  accessible |
| **Documentation** | 10% | Clear README, comments, API docs |
| **UI/UX** | 5% | User-friendly interface |

---

## ğŸ’¡ Chi Tiáº¿t Model & Data Preprocessing

### Linear Regression Enhancement Options

#### Option 1: Ridge Regression (L2 Regularization)
```python
from sklearn.linear_model import Ridge
model = Ridge(alpha=1.0)
```
- **Advantages:** Giáº£m overfitting, stable vá»›i multicollinearity
- **Use case:** Khi features cÃ³ correlation cao

#### Option 2: Lasso Regression (L1 Regularization)
```python
from sklearn.linear_model import Lasso
model = Lasso(alpha=1.0)
```
- **Advantages:** Feature selection tá»± Ä‘á»™ng, sparse model
- **Use case:** Khi cÃ³ nhiá»u features khÃ´ng quan trá»ng

#### Option 3: ElasticNet (L1 + L2)
```python
from sklearn.linear_model import ElasticNet
model = ElasticNet(alpha=1.0, l1_ratio=0.5)
```
- **Advantages:** Káº¿t há»£p Æ°u Ä‘iá»ƒm cá»§a Ridge vÃ  Lasso
- **Use case:** Balanced approach

#### Option 4: Polynomial Regression
```python
from sklearn.preprocessing import PolynomialFeatures
from sklearn.linear_model import LinearRegression
poly = PolynomialFeatures(degree=2)
model = LinearRegression()
```
- **Advantages:** Capture non-linear relationships
- **Use case:** Khi relationship khÃ´ng hoÃ n toÃ n linear

---

### ğŸ”§ Data Preprocessing Requirements (CRITICAL)

> [!IMPORTANT]
> Dá»¯ liá»‡u cÃ³ missing values - preprocessing lÃ  báº¯t buá»™c!

#### 1. Missing Data Handling

**PhÆ°Æ¡ng phÃ¡p Fill Missing Values:**

```python
import pandas as pd
from sklearn.impute import SimpleImputer

# Option A: Mean/Median Imputation
imputer = SimpleImputer(strategy='mean')  # hoáº·c 'median'
X_filled = imputer.fit_transform(X)

# Option B: Forward/Backward Fill (náº¿u cÃ³ time series)
df.fillna(method='ffill')  # forward fill
df.fillna(method='bfill')  # backward fill

# Option C: KNN Imputer (advanced)
from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
X_filled = imputer.fit_transform(X)
```

**Báº¯t buá»™c pháº£i lÃ m:**
- âœ… Detect missing values: `df.isnull().sum()`
- âœ… Visualize missing pattern: `import missingno as msno`
- âœ… Choose appropriate imputation strategy
- âœ… Document imputation method trong code

#### 2. Feature Scaling

```python
from sklearn.preprocessing import StandardScaler, MinMaxScaler

# StandardScaler (recommended for Linear Regression)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X_filled)

# MinMaxScaler (alternative)
scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(X_filled)
```

#### 3. Outlier Detection (Optional but Recommended)

```python
from scipy import stats
import numpy as np

# Z-score method
z_scores = np.abs(stats.zscore(X))
X_no_outliers = X[(z_scores < 3).all(axis=1)]

# IQR method
Q1 = X.quantile(0.25)
Q3 = X.quantile(0.75)
IQR = Q3 - Q1
X_no_outliers = X[~((X < (Q1 - 1.5 * IQR)) | (X > (Q3 + 1.5 * IQR))).any(axis=1)]
```

#### 4. Data Validation

```python
# Ensure all data is float64
assert X.dtypes.all() == 'float64'

# Check no missing values after imputation
assert X_filled.isnull().sum().sum() == 0

# Check data shape
print(f"Features: {X.shape[1]}, Samples: {X.shape[0]}")
```

---

### ğŸ¯ Model Validation & Selection (CRITICAL)

> [!IMPORTANT]
> Model validation Ä‘áº£m báº£o model cÃ³ hiá»‡u suáº¥t tá»‘t nháº¥t vÃ  khÃ´ng bá»‹ overfitting!

#### 1. Train-Test Split vá»›i Random State Cá»‘ Äá»‹nh

```python
from sklearn.model_selection import train_test_split

# CRITICAL: Always use random_state=42 for reproducibility
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, 
    test_size=0.2,  # 80-20 split
    random_state=42  # âš ï¸ Cá» Äá»ŠNH = 42
)

print(f"Training set: {X_train.shape[0]} samples")
print(f"Test set: {X_test.shape[0]} samples")
```

**LÃ½ do random_state=42:**
- âœ… Reproducibility: Káº¿t quáº£ giá»‘ng nhau má»—i láº§n cháº¡y
- âœ… Debugging: Dá»… dÃ ng so sÃ¡nh vÃ  debug
- âœ… Collaboration: Team members cÃ³ cÃ¹ng káº¿t quáº£

#### 2. Cross-Validation

```python
from sklearn.model_selection import cross_val_score, cross_validate
from sklearn.linear_model import Ridge, Lasso, ElasticNet
import numpy as np

# Define models to compare
models = {
    'Linear Regression': LinearRegression(),
    'Ridge': Ridge(random_state=42),
    'Lasso': Lasso(random_state=42),
    'ElasticNet': ElasticNet(random_state=42)
}

# Perform 5-fold cross-validation
for name, model in models.items():
    # Use cv=5 or cv=10 for k-fold
    scores = cross_val_score(
        model, X_train, y_train, 
        cv=5,  # 5-fold cross-validation
        scoring='r2',  # or 'neg_mean_squared_error'
        n_jobs=-1  # Use all CPU cores
    )
    
    print(f"{name}:")
    print(f"  Mean RÂ² Score: {scores.mean():.4f}")
    print(f"  Std Dev: {scores.std():.4f}")
    print(f"  Scores: {scores}")
```

**Cross-validation metrics to track:**
- RÂ² Score (coefficient of determination)
- MAE (Mean Absolute Error)
- MSE (Mean Squared Error)
- RMSE (Root Mean Squared Error)

#### 3. Hyperparameter Tuning vá»›i GridSearchCV

```python
from sklearn.model_selection import GridSearchCV

# Example: Tune Ridge Regression
param_grid = {
    'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]
}

ridge = Ridge(random_state=42)

grid_search = GridSearchCV(
    ridge,
    param_grid,
    cv=5,
    scoring='r2',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best CV score: {grid_search.best_score_:.4f}")

# Use best model
best_model = grid_search.best_estimator_
```

#### 4. Model Evaluation on Test Set

```python
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Make predictions
y_pred = best_model.predict(X_test)

# Calculate metrics
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_test, y_pred)

print("\n=== Test Set Performance ===")
print(f"MAE:  {mae:.4f}")
print(f"MSE:  {mse:.4f}")
print(f"RMSE: {rmse:.4f}")
print(f"RÂ² Score: {r2:.4f}")
```

#### 5. Validation Report Template

```python
import pandas as pd

# Create validation report
validation_report = {
    'Model': ['Linear Regression', 'Ridge', 'Lasso', 'ElasticNet'],
    'CV_Mean_R2': [],  # From cross-validation
    'CV_Std_R2': [],
    'Test_R2': [],     # From test set
    'Test_MAE': [],
    'Test_RMSE': []
}

df_report = pd.DataFrame(validation_report)
df_report.to_csv('model_validation_report.csv', index=False)
print(df_report)
```

#### 6. Overfitting Detection

```python
# Compare training vs test performance
train_score = best_model.score(X_train, y_train)
test_score = best_model.score(X_test, y_test)

print(f"Training RÂ² Score: {train_score:.4f}")
print(f"Test RÂ² Score: {test_score:.4f}")
print(f"Difference: {abs(train_score - test_score):.4f}")

if abs(train_score - test_score) > 0.1:
    print("âš ï¸ WARNING: Possible overfitting detected!")
else:
    print("âœ… Model generalizes well")
```

#### 7. Learning Curve (Optional but Recommended)

```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

train_sizes, train_scores, val_scores = learning_curve(
    best_model, X_train, y_train,
    cv=5,
    n_jobs=-1,
    train_sizes=np.linspace(0.1, 1.0, 10),
    random_state=42
)

# Plot learning curves
plt.figure(figsize=(10, 6))
plt.plot(train_sizes, train_scores.mean(axis=1), label='Training score')
plt.plot(train_sizes, val_scores.mean(axis=1), label='Validation score')
plt.xlabel('Training Set Size')
plt.ylabel('RÂ² Score')
plt.title('Learning Curves')
plt.legend()
plt.grid(True)
plt.savefig('learning_curves.png')
```

**Validation Checklist:**
- âœ… random_state=42 used in all random operations
- âœ… Cross-validation performed (k=5 or k=10)
- âœ… Multiple models compared
- âœ… Hyperparameters tuned
- âœ… Test set evaluation completed
- âœ… Overfitting checked
- âœ… Validation report generated

---

### ğŸ“Š Suggested Datasets (Float64 Numerical Only)

1. **California Housing Dataset**
   - 8 features, all numerical
   - Predict median house value
   ```python
   from sklearn.datasets import fetch_california_housing
   data = fetch_california_housing()
   ```

2. **Boston Housing Dataset** (Classic)
   - 13 features, all numerical
   - Predict house prices
   ```python
   # Note: Deprecated in sklearn, use alternative
   import pandas as pd
   url = "http://lib.stat.cmu.edu/datasets/boston"
   ```

3. **Diabetes Dataset**
   - 10 features, all numerical
   - Predict disease progression
   ```python
   from sklearn.datasets import load_diabetes
   data = load_diabetes()
   ```

4. **Custom Dataset**
   - CSV file vá»›i all float64 columns
   - Manually introduce missing values Ä‘á»ƒ practice preprocessing

---

## ğŸ“¦ Python Requirements (requirements.txt)

```txt
# Core dependencies
Flask==3.0.0
gunicorn==21.2.0

# ML Libraries
scikit-learn==1.3.0
pandas==2.0.3
numpy==1.24.3
joblib==1.3.2

# Data preprocessing
scipy==1.11.2

# Optional: Visualization trong notebooks
matplotlib==3.7.2
seaborn==0.12.2
missingno==0.5.2

# Testing
pytest==7.4.0

# CORS support
flask-cors==4.0.0

# Environment variables
python-dotenv==1.0.0
```

> [!TIP]
> Install vá»›i: `pip install -r requirements.txt`

---

## ğŸ“š Resources & References

### Learning Resources
- **Flask:** https://flask.palletsprojects.com/
- **FastAPI:** https://fastapi.tiangolo.com/
- **Responsive Design:** https://web.dev/responsive-web-design-basics/
- **Deployment:** 
  - Render: https://render.com/docs
  - Railway: https://docs.railway.app/

### Python Libraries Documentation
- TensorFlow: https://www.tensorflow.org/
- PyTorch: https://pytorch.org/
- Scikit-learn: https://scikit-learn.org/

---

## âš ï¸ Common Pitfalls to Avoid

1. âŒ Model quÃ¡ lá»›n, inference cháº­m
2. âŒ KhÃ´ng test responsive Ä‘áº§y Ä‘á»§
3. âŒ Hardcode credentials trong code
4. âŒ KhÃ´ng handle errors properly
5. âŒ CORS issues khi deploy
6. âŒ KhÃ´ng optimize images
7. âŒ QuÃªn setup environment variables

---

## âœ… Success Criteria

Project Ä‘Æ°á»£c coi lÃ  thÃ nh cÃ´ng khi:

- âœ… Website accessible qua public URL
- âœ… Model prediction hoáº¡t Ä‘á»™ng chÃ­nh xÃ¡c
- âœ… Responsive trÃªn mobile, tablet, desktop
- âœ… API response time há»£p lÃ½
- âœ… Code Ä‘Æ°á»£c commit lÃªn GitHub
- âœ… Documentation Ä‘áº§y Ä‘á»§
- âœ… Demo thÃ nh cÃ´ng

---

## ğŸ“ Next Steps

1. **Review document nÃ y** vÃ  Ä‘áº£m báº£o hiá»ƒu rÃµ requirements
2. **Chá»n AI model cá»¥ thá»ƒ** tá»« cÃ¡c gá»£i Ã½ trÃªn
3. **Setup development environment** (Python, IDE, Git)
4. **Báº¯t Ä‘áº§u Phase 1** theo Development Workflow
5. **Track progress** vÃ  update theo checklist

---

**Good luck with your project! ğŸš€**

*Document nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c update trong quÃ¡ trÃ¬nh development khi cÃ³ thÃªm requirements má»›i.*
